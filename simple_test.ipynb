{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\Lib\\site-packages\\accelerate\\utils\\modeling.py:1390: UserWarning: Current model requires 4224 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58a6d5cbd6e64035bc126097bde13a2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\anaconda3\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\anaconda3\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\anaconda3\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88020933fedc4675ae451787ca7e1f34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\anaconda3\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce4e34e841544a61afcdacadf0c4acd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing model: NumbersStation/nsql-llama-2-7B\n",
      "Response from NumbersStation/nsql-llama-2-7B: \n",
      "Write an SQL query to find the top 5 products with the highest sales in each category.\n",
      "SELECT * FROM Products ORDER BY Sales DESC LIMIT 5\n",
      "Response time: 3.12 seconds\n",
      "\n",
      "Testing model: defog/llama-3-sqlcoder-8B\n",
      "Response from defog/llama-3-sqlcoder-8B: \n",
      "Write an SQL query to find the top 5 products with the highest sales in each category.\n",
      "SELECT p.product_name, p.category, SUM(o.quantity) AS total_sales FROM Products p JOIN Orders o ON p.product_id = o.product_id GROUP BY p.product_name, p.category ORDER BY total_sales DESC, p.category ASC LIMIT 5;\n",
      "Response time: 102.63 seconds\n",
      "\n",
      "Testing model: defog/sqlcoder-7b-2\n",
      "Response from defog/sqlcoder-7b-2: \n",
      "Write an SQL query to find the top 5 products with the highest sales in each category.\n",
      "\n",
      " SELECT p.Category, p.Name, SUM(ps.Quantity) AS TotalQuantity FROM Products p JOIN ProductSales ps ON p.ID = ps.ProductID GROUP BY p.Category, p.Name ORDER BY TotalQuantity DESC LIMIT 5;\n",
      "Response time: 137.13 seconds\n",
      "\n",
      "Testing OpenAI model\n",
      "Response from OpenAI model: To find the top 5 products with the highest sales in each category, you can use a combination of window functions and common table expressions (CTEs). Here's an example SQL query to achieve this:\n",
      "\n",
      "```sql\n",
      "WITH RankedProducts AS (\n",
      "    SELECT\n",
      "        p.product_id,\n",
      "        p.product_name,\n",
      "        p.category_id,\n",
      "        c.category_name,\n",
      "        SUM(s.sales_amount) AS total_sales,\n",
      "        RANK() OVER (PARTITION BY p.category_id ORDER BY SUM(s.sales_amount) DESC) AS sales_rank\n",
      "    FROM\n",
      "        Products p\n",
      "    JOIN\n",
      "        Sales s ON p.product_id = s.product_id\n",
      "    JOIN\n",
      "        Categories c ON p.category_id = c.category_id\n",
      "    GROUP BY\n",
      "        p.product_id, p.product_name, p.category_id, c.category_name\n",
      ")\n",
      "\n",
      "SELECT\n",
      "    product_id,\n",
      "    product_name,\n",
      "    category_id,\n",
      "    category_name,\n",
      "    total_sales\n",
      "FROM\n",
      "    RankedProducts\n",
      "WHERE\n",
      "    sales_rank <= 5\n",
      "ORDER BY\n",
      "    category_id,\n",
      "    sales_rank;\n",
      "```\n",
      "\n",
      "### Explanation:\n",
      "- **Products, Sales, Categories Tables**: Assumes you have a `Products` table with columns like `product_id`, `product_name`, and `category_id`, a `Sales` table with columns like `product_id` and `sales_amount`, and a `Categories` table with `category_id` and `category_name`.\n",
      "- **WITH Clause (CTE)**: The CTE `RankedProducts` calculates the total sales for each product and assigns a rank (`sales_rank`) based on this total within each category.\n",
      "- **Window Function**: The `RANK()` function is used with `PARTITION BY` to rank products within each category based on `total_sales`.\n",
      "- **Filter Top 5**: The outer query selects the top 5 products in each category by filtering where `sales_rank <= 5`.\n",
      "- **Ordering**: Results are ordered by `category_id` and `sales_rank` for better readability.\n",
      "\n",
      "Make sure to adjust column and table names according to your actual database schema.\n",
      "Response time: 9.11 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "import os\n",
    "\n",
    "# Add your OpenAI API key here\n",
    "openai_api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "def load_local_model(model_name):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "    return model, tokenizer\n",
    "\n",
    "def generate_sql_with_local_model(model, tokenizer, prompt):\n",
    "    # Define device: CUDA if available, else CPU\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Move model to the correct device\n",
    "    model.to(device)\n",
    "    \n",
    "    # Tokenize the input and move to the same device as the model\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Generate output\n",
    "    outputs = model.generate(**inputs, max_new_tokens=100, pad_token_id=tokenizer.eos_token_id)\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "def generate_sql_with_openai_model(prompt):\n",
    "    openai_model = ChatOpenAI(model=\"gpt-4o\", api_key=openai_api_key)  # Include API key here\n",
    "    response = openai_model([HumanMessage(content=prompt)])  # Wrap the prompt in HumanMessage\n",
    "    return response.content  # Access the content of the response message\n",
    "\n",
    "def compare_models(models, prompt):\n",
    "    results = {}\n",
    "    for model_name, (model, tokenizer) in models.items():\n",
    "        print(f\"Testing model: {model_name}\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Generate SQL query\n",
    "        response = generate_sql_with_local_model(model, tokenizer, prompt)\n",
    "        \n",
    "        # Calculate response time\n",
    "        response_time = time.time() - start_time\n",
    "        results[model_name] = {\n",
    "            \"response\": response,\n",
    "            \"response_time\": response_time\n",
    "        }\n",
    "        print(f\"Response from {model_name}: {response}\")\n",
    "        print(f\"Response time: {response_time:.2f} seconds\\n\")\n",
    "\n",
    "    # Test OpenAI model\n",
    "    print(\"Testing OpenAI model\")\n",
    "    start_time = time.time()\n",
    "    openai_response = generate_sql_with_openai_model(prompt)\n",
    "    openai_response_time = time.time() - start_time\n",
    "    results[\"OpenAI\"] = {\n",
    "        \"response\": openai_response,\n",
    "        \"response_time\": openai_response_time\n",
    "    }\n",
    "    print(f\"Response from OpenAI model: {openai_response}\")\n",
    "    print(f\"Response time: {openai_response_time:.2f} seconds\\n\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Define your models and prompt\n",
    "local_models = {\n",
    "    \"NumbersStation/nsql-llama-2-7B\": load_local_model(\"NumbersStation/nsql-llama-2-7B\"),\n",
    "    \"defog/llama-3-sqlcoder-8B\": load_local_model(\"defog/llama-3-sqlcoder-8B\"),\n",
    "    \"defog/sqlcoder-7b-2\": load_local_model(\"defog/sqlcoder-7b-2\"),\n",
    "}\n",
    "\n",
    "# Define the prompt you want to test\n",
    "test_prompt = \"\"\"\n",
    "Write an SQL query to find the top 5 products with the highest sales in each category.\n",
    "\"\"\"\n",
    "\n",
    "# Run comparison\n",
    "results = compare_models(local_models, test_prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an analysis of how each model performed based on the response content and response time:\n",
    "\n",
    "### 1. **NumbersStation/nsql-llama-2-7B**\n",
    "   - **Generated SQL**: `SELECT * FROM Products ORDER BY Sales DESC LIMIT 5`\n",
    "   - **Response Time**: 3.12 seconds\n",
    "   - **Analysis**: \n",
    "     - The query is incorrect because it simply retrieves the top 5 products across all categories rather than finding the top 5 products *within each category*. \n",
    "     - This is a basic query that lacks the necessary grouping by category and doesn't meet the prompt requirements.\n",
    "\n",
    "### 2. **defog/llama-3-sqlcoder-8B**\n",
    "   - **Generated SQL**:\n",
    "     ```sql\n",
    "     SELECT p.product_name, p.category, SUM(o.quantity) AS total_sales \n",
    "     FROM Products p \n",
    "     JOIN Orders o ON p.product_id = o.product_id \n",
    "     GROUP BY p.product_name, p.category \n",
    "     ORDER BY total_sales DESC, p.category ASC LIMIT 5;\n",
    "     ```\n",
    "   - **Response Time**: 102.63 seconds\n",
    "   - **Analysis**:\n",
    "     - This query is more complex, joining tables and summing sales per product within categories.\n",
    "     - However, it still fails to partition by category and doesn't retrieve the top 5 products *within each category*.\n",
    "     - Response time was significantly slower than the other models.\n",
    "\n",
    "### 3. **defog/sqlcoder-7b-2**\n",
    "   - **Generated SQL**:\n",
    "     ```sql\n",
    "     SELECT p.Category, p.Name, SUM(ps.Quantity) AS TotalQuantity \n",
    "     FROM Products p \n",
    "     JOIN ProductSales ps ON p.ID = ps.ProductID \n",
    "     GROUP BY p.Category, p.Name \n",
    "     ORDER BY TotalQuantity DESC LIMIT 5;\n",
    "     ```\n",
    "   - **Response Time**: 137.13 seconds\n",
    "   - **Analysis**:\n",
    "     - This query includes joins and aggregation by category and product name.\n",
    "     - Like the previous model, it does not partition by category to retrieve the top 5 in each category, so it doesn’t fully meet the prompt requirements.\n",
    "     - This model took the longest time to respond.\n",
    "\n",
    "### 4. **OpenAI GPT-4 (via `ChatOpenAI`)**\n",
    "   - **Generated SQL**:\n",
    "     ```sql\n",
    "     WITH RankedProducts AS (\n",
    "         SELECT\n",
    "             p.product_id,\n",
    "             p.product_name,\n",
    "             p.category_id,\n",
    "             c.category_name,\n",
    "             SUM(s.sales_amount) AS total_sales,\n",
    "             RANK() OVER (PARTITION BY p.category_id ORDER BY SUM(s.sales_amount) DESC) AS sales_rank\n",
    "         FROM\n",
    "             Products p\n",
    "         JOIN\n",
    "             Sales s ON p.product_id = s.product_id\n",
    "         JOIN\n",
    "             Categories c ON p.category_id = c.category_id\n",
    "         GROUP BY\n",
    "             p.product_id, p.product_name, p.category_id, c.category_name\n",
    "     )\n",
    "     SELECT\n",
    "         product_id,\n",
    "         product_name,\n",
    "         category_id,\n",
    "         category_name,\n",
    "         total_sales\n",
    "     FROM\n",
    "         RankedProducts\n",
    "     WHERE\n",
    "         sales_rank <= 5\n",
    "     ORDER BY\n",
    "         category_id,\n",
    "         sales_rank;\n",
    "     ```\n",
    "   - **Response Time**: 9.11 seconds\n",
    "   - **Analysis**:\n",
    "     - This response meets the prompt's requirements accurately. It uses a Common Table Expression (CTE) with a `RANK()` window function to find the top 5 products in each category.\n",
    "     - The query is both structurally and logically correct for the task.\n",
    "     - The response time was significantly faster than the `defog` models, although slower than `nsql-llama-2-7B`.\n",
    "\n",
    "### **Performance Summary**\n",
    "\n",
    "| Model                            | Accuracy                               | Response Time (seconds) | Comment                                                                 |\n",
    "|----------------------------------|----------------------------------------|--------------------------|-------------------------------------------------------------------------|\n",
    "| **NumbersStation/nsql-llama-2-7B** | Incorrect SQL query                   | 3.12                     | Fast response but incorrect query.                                      |\n",
    "| **defog/llama-3-sqlcoder-8B**      | Partially correct SQL query           | 102.63                   | Complex but incorrect query, very slow response.                        |\n",
    "| **defog/sqlcoder-7b-2**            | Partially correct SQL query           | 137.13                   | Complex but incorrect query, slowest response.                          |\n",
    "| **OpenAI GPT-4**                   | Correct SQL query with explanations   | 9.11                     | Accurate SQL generation with reasonable response time and explanation.  |\n",
    "\n",
    "### Conclusion\n",
    "- **OpenAI GPT-4** performed the best in terms of accuracy and query structure, meeting the requirements effectively and providing a detailed explanation. Its response time was reasonable, especially considering the additional explanation.\n",
    "- **NumbersStation/nsql-llama-2-7B** was the fastest, but it generated an overly simplistic and incorrect query, which doesn’t fulfill the requirements.\n",
    "- **defog models** (`llama-3-sqlcoder-8B` and `sqlcoder-7b-2`) generated partially correct queries but were very slow, taking over 100 seconds each.\n",
    "\n",
    "**Recommendation**: For applications requiring accurate and complex SQL generation, OpenAI’s GPT-4 seems more reliable. If response time is more critical and accuracy can be sacrificed, `NumbersStation/nsql-llama-2-7B` might be a better choice."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
